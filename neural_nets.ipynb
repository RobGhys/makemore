{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Using Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T13:01:09.531096101Z",
     "start_time": "2023-07-20T13:01:08.448631256Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a .\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        print(ch1, ch2)\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When 0 is inputed (xs), we want 5 to have a high probability (ys),\n",
    "\n",
    "When 5 is input, we want 13,\n",
    "\n",
    "etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encoding, converting from int64 to float32\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs 1 in yellow and 0 in purple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2f006de080>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy example with only 1 Neuron (27, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3675],\n",
       "        [0.2182],\n",
       "        [0.1128],\n",
       "        [0.1146],\n",
       "        [0.6597],\n",
       "        [0.6873],\n",
       "        [0.9078],\n",
       "        [0.1295],\n",
       "        [0.6714],\n",
       "        [0.5776],\n",
       "        [0.5652],\n",
       "        [0.5796],\n",
       "        [0.3261],\n",
       "        [0.6189],\n",
       "        [0.3169],\n",
       "        [0.4168],\n",
       "        [0.4263],\n",
       "        [0.7474],\n",
       "        [0.9784],\n",
       "        [0.7467],\n",
       "        [0.1237],\n",
       "        [0.9831],\n",
       "        [0.5542],\n",
       "        [0.8203],\n",
       "        [0.4622],\n",
       "        [0.9170],\n",
       "        [0.2502]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A column Vector\n",
    "W = torch.rand((27, 1))\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5, 27) @ (27, 1) --> (5, 1)\n",
    "mmul = xenc @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3675],\n",
       "        [0.6873],\n",
       "        [0.6189],\n",
       "        [0.6189],\n",
       "        [0.2182]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output: 5 activations\n",
    "mmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27 Neurons now (27, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9366, 0.5190, 0.9764, 0.4486, 0.5554, 0.8981, 0.4531, 0.0479, 0.6779,\n",
       "         0.7846, 0.6378, 0.1561, 0.3133, 0.0240, 0.6037, 0.0444, 0.8018, 0.6966,\n",
       "         0.0090, 0.5997, 0.1699, 0.8836, 0.8057, 0.3450, 0.7114, 0.8646, 0.0089],\n",
       "        [0.9135, 0.8817, 0.5788, 0.2431, 0.3961, 0.7055, 0.1113, 0.5795, 0.7377,\n",
       "         0.3734, 0.4681, 0.4328, 0.2073, 0.2705, 0.8959, 0.4624, 0.9733, 0.1387,\n",
       "         0.2958, 0.1278, 0.9244, 0.7165, 0.0402, 0.8755, 0.4809, 0.6072, 0.4759],\n",
       "        [0.7181, 0.9653, 0.3309, 0.8266, 0.1296, 0.3340, 0.7018, 0.0813, 0.1959,\n",
       "         0.0732, 0.9118, 0.8659, 0.3878, 0.9871, 0.1771, 0.1730, 0.3598, 0.2156,\n",
       "         0.1981, 0.6877, 0.8399, 0.7574, 0.1268, 0.0167, 0.7191, 0.9575, 0.2276],\n",
       "        [0.2487, 0.4109, 0.2606, 0.8658, 0.4657, 0.5404, 0.4052, 0.3889, 0.9796,\n",
       "         0.3006, 0.7508, 0.3038, 0.2279, 0.7442, 0.4300, 0.1646, 0.6438, 0.2670,\n",
       "         0.6080, 0.0747, 0.2142, 0.7981, 0.1193, 0.2860, 0.4178, 0.8032, 0.8103],\n",
       "        [0.5840, 0.4357, 0.7520, 0.4363, 0.8707, 0.2372, 0.2630, 0.7579, 0.5565,\n",
       "         0.2982, 0.8884, 0.0283, 0.7421, 0.2324, 0.1749, 0.9934, 0.2454, 0.3320,\n",
       "         0.1650, 0.5867, 0.4140, 0.2249, 0.3433, 0.8313, 0.5473, 0.5544, 0.0073],\n",
       "        [0.1258, 0.2447, 0.1917, 0.0778, 0.9824, 0.9961, 0.1257, 0.9962, 0.7482,\n",
       "         0.0694, 0.9248, 0.9177, 0.3504, 0.9840, 0.6242, 0.0179, 0.9180, 0.0224,\n",
       "         0.0084, 0.0039, 0.1443, 0.7109, 0.4794, 0.4594, 0.6280, 0.5412, 0.8272],\n",
       "        [0.3380, 0.7796, 0.8702, 0.7721, 0.4921, 0.5931, 0.2765, 0.6200, 0.8482,\n",
       "         0.2822, 0.9165, 0.9097, 0.3989, 0.4744, 0.7940, 0.9787, 0.4124, 0.4842,\n",
       "         0.8936, 0.2015, 0.7150, 0.7505, 0.5760, 0.6973, 0.5108, 0.6198, 0.1105],\n",
       "        [0.5741, 0.1742, 0.8198, 0.4742, 0.8576, 0.3999, 0.6889, 0.6385, 0.4289,\n",
       "         0.7545, 0.6950, 0.3816, 0.4088, 0.1062, 0.7736, 0.4365, 0.3775, 0.3154,\n",
       "         0.3791, 0.3311, 0.2438, 0.1041, 0.9793, 0.9250, 0.1619, 0.8647, 0.5709],\n",
       "        [0.9480, 0.3552, 0.6125, 0.0417, 0.4191, 0.7207, 0.5349, 0.7662, 0.6701,\n",
       "         0.8520, 0.4115, 0.5875, 0.4656, 0.1283, 0.7651, 0.2830, 0.0730, 0.7487,\n",
       "         0.4064, 0.6715, 0.4999, 0.2932, 0.7631, 0.1684, 0.1105, 0.8267, 0.8228],\n",
       "        [0.3211, 0.9865, 0.6684, 0.5916, 0.6558, 0.8418, 0.7494, 0.3942, 0.5055,\n",
       "         0.6317, 0.8092, 0.5936, 0.0919, 0.2964, 0.8069, 0.9356, 0.4854, 0.3513,\n",
       "         0.4482, 0.3045, 0.4446, 0.5147, 0.6837, 0.4087, 0.8009, 0.5222, 0.6833],\n",
       "        [0.1374, 0.4612, 0.1910, 0.3489, 0.7453, 0.1602, 0.9285, 0.9248, 0.6325,\n",
       "         0.1818, 0.3229, 0.2634, 0.1924, 0.5639, 0.7125, 0.6222, 0.2238, 0.0930,\n",
       "         0.0336, 0.1335, 0.0973, 0.6215, 0.5238, 0.1626, 0.1825, 0.8837, 0.9786],\n",
       "        [0.7152, 0.8709, 0.6815, 0.6169, 0.1378, 0.6759, 0.9678, 0.0965, 0.7460,\n",
       "         0.3631, 0.4454, 0.3415, 0.8602, 0.5612, 0.1604, 0.0559, 0.3052, 0.2452,\n",
       "         0.8784, 0.4167, 0.8152, 0.3024, 0.2848, 0.7894, 0.8020, 0.7269, 0.0049],\n",
       "        [0.1695, 0.3050, 0.2717, 0.5905, 0.1794, 0.4007, 0.0479, 0.0079, 0.5996,\n",
       "         0.1107, 0.6343, 0.2769, 0.9500, 0.9904, 0.5260, 0.0339, 0.0120, 0.6519,\n",
       "         0.0730, 0.1661, 0.8971, 0.1931, 0.0080, 0.4475, 0.7817, 0.6867, 0.6168],\n",
       "        [0.4077, 0.1372, 0.5140, 0.0969, 0.1731, 0.7660, 0.2043, 0.8372, 0.4805,\n",
       "         0.5406, 0.1567, 0.4494, 0.3641, 0.3795, 0.4450, 0.4580, 0.8718, 0.3630,\n",
       "         0.4909, 0.6181, 0.3448, 0.0902, 0.2372, 0.7876, 0.4160, 0.1699, 0.2071],\n",
       "        [0.4259, 0.3289, 0.9339, 0.0226, 0.5015, 0.0269, 0.4076, 0.8751, 0.6549,\n",
       "         0.9026, 0.7255, 0.3455, 0.4903, 0.6561, 0.8361, 0.4879, 0.1263, 0.3182,\n",
       "         0.1444, 0.6266, 0.1834, 0.7464, 0.2571, 0.1080, 0.1843, 0.4940, 0.4148],\n",
       "        [0.3394, 0.5985, 0.5238, 0.5363, 0.4218, 0.3860, 0.0399, 0.5994, 0.9890,\n",
       "         0.8570, 0.2890, 0.1289, 0.2958, 0.7391, 0.9757, 0.7295, 0.0169, 0.0364,\n",
       "         0.1525, 0.6383, 0.8581, 0.7413, 0.2267, 0.5316, 0.4946, 0.3571, 0.3498],\n",
       "        [0.5319, 0.7399, 0.6596, 0.6175, 0.5400, 0.8030, 0.9918, 0.9542, 0.5189,\n",
       "         0.3521, 0.4979, 0.2731, 0.5246, 0.3388, 0.4065, 0.6078, 0.2128, 0.8124,\n",
       "         0.2628, 0.0054, 0.0524, 0.9755, 0.4838, 0.3908, 0.4958, 0.5881, 0.4336],\n",
       "        [0.7105, 0.3915, 0.3170, 0.0554, 0.6838, 0.7775, 0.1481, 0.9621, 0.4404,\n",
       "         0.6605, 0.9989, 0.4323, 0.9641, 0.3140, 0.2406, 0.0785, 0.4427, 0.5740,\n",
       "         0.8147, 0.0534, 0.6322, 0.5203, 0.3041, 0.9062, 0.8392, 0.4430, 0.5622],\n",
       "        [0.7489, 0.6157, 0.2748, 0.3475, 0.8732, 0.2414, 0.8079, 0.7242, 0.7521,\n",
       "         0.4704, 0.0011, 0.4980, 0.9732, 0.1107, 0.7394, 0.0678, 0.9134, 0.9708,\n",
       "         0.6880, 0.5089, 0.4873, 0.1372, 0.1001, 0.1412, 0.1761, 0.9753, 0.3925],\n",
       "        [0.5019, 0.9160, 0.5999, 0.0458, 0.6917, 0.7210, 0.9709, 0.1609, 0.5648,\n",
       "         0.4062, 0.0817, 0.0154, 0.4738, 0.4646, 0.4343, 0.0084, 0.7041, 0.8899,\n",
       "         0.1411, 0.5625, 0.5471, 0.5211, 0.4811, 0.7633, 0.7947, 0.2668, 0.7910],\n",
       "        [0.1098, 0.4560, 0.1086, 0.3750, 0.9094, 0.8877, 0.0934, 0.0069, 0.8236,\n",
       "         0.6361, 0.1127, 0.5247, 0.1912, 0.1829, 0.4961, 0.8395, 0.6546, 0.4606,\n",
       "         0.5817, 0.4950, 0.0168, 0.1337, 0.6410, 0.3312, 0.6344, 0.5000, 0.3815],\n",
       "        [0.1379, 0.1994, 0.7064, 0.6652, 0.0646, 0.3487, 0.8413, 0.6319, 0.2307,\n",
       "         0.7856, 0.0027, 0.4947, 0.3244, 0.1857, 0.5444, 0.9010, 0.8856, 0.7951,\n",
       "         0.2535, 0.8937, 0.2402, 0.0059, 0.7451, 0.5554, 0.4941, 0.4698, 0.5402],\n",
       "        [0.0366, 0.9381, 0.7189, 0.3304, 0.8823, 0.3874, 0.0522, 0.9866, 0.7249,\n",
       "         0.1773, 0.5689, 0.1332, 0.9395, 0.3756, 0.3252, 0.3978, 0.2675, 0.1217,\n",
       "         0.9515, 0.7971, 0.5834, 0.4145, 0.3578, 0.3267, 0.5742, 0.0323, 0.3357],\n",
       "        [0.9253, 0.6902, 0.1548, 0.6420, 0.6123, 0.4693, 0.9815, 0.7273, 0.0024,\n",
       "         0.2858, 0.6579, 0.8337, 0.7129, 0.1137, 0.4345, 0.0953, 0.8085, 0.2299,\n",
       "         0.3446, 0.3643, 0.8045, 0.6575, 0.0060, 0.9375, 0.8499, 0.6863, 0.1440],\n",
       "        [0.3118, 0.2885, 0.9316, 0.9615, 0.3469, 0.1448, 0.2050, 0.1904, 0.9790,\n",
       "         0.2967, 0.7877, 0.5604, 0.1184, 0.2342, 0.3817, 0.0633, 0.0623, 0.8807,\n",
       "         0.4604, 0.6083, 0.0476, 0.3605, 0.4497, 0.5535, 0.2519, 0.3523, 0.3854],\n",
       "        [0.9932, 0.2921, 0.1655, 0.3793, 0.9869, 0.6505, 0.5925, 0.7795, 0.9122,\n",
       "         0.5073, 0.2456, 0.9612, 0.4618, 0.2285, 0.9842, 0.4369, 0.0443, 0.6229,\n",
       "         0.9778, 0.0927, 0.8425, 0.2321, 0.9297, 0.5159, 0.5587, 0.6901, 0.3185],\n",
       "        [0.2385, 0.7199, 0.1925, 0.7800, 0.3410, 0.7357, 0.4280, 0.5674, 0.7142,\n",
       "         0.9103, 0.9515, 0.6717, 0.8735, 0.4539, 0.4618, 0.2947, 0.9833, 0.8648,\n",
       "         0.4004, 0.2817, 0.3367, 0.0245, 0.9346, 0.4358, 0.1607, 0.4886, 0.1701]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.rand((27, 27))\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 27]), torch.Size([27, 27]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape, W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (5, 27) @ (27, 27) -> (5, 27)\n",
    "mmul = xenc @ W\n",
    "mmul.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3795)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Firing rate of the 13th neuron (i.e. 13th column of W), looking at the 3rd input (i.e. of xenc)\n",
    "(xenc @ W)[3, 13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can do this matrix multiplication step-by-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc_3 = xenc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_13 = W[:, 13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the same result as the one from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3795)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc_3 * W_13).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict output probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative numbers are smaller than 1.\n",
    "\n",
    "Positive numbers are greater than 1.\n",
    "\n",
    "Thanks to the exp, we can count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9366, 0.5190, 0.9764, 0.4486, 0.5554, 0.8981, 0.4531, 0.0479, 0.6779,\n",
       "         0.7846, 0.6378, 0.1561, 0.3133, 0.0240, 0.6037, 0.0444, 0.8018, 0.6966,\n",
       "         0.0090, 0.5997, 0.1699, 0.8836, 0.8057, 0.3450, 0.7114, 0.8646, 0.0089],\n",
       "        [0.1258, 0.2447, 0.1917, 0.0778, 0.9824, 0.9961, 0.1257, 0.9962, 0.7482,\n",
       "         0.0694, 0.9248, 0.9177, 0.3504, 0.9840, 0.6242, 0.0179, 0.9180, 0.0224,\n",
       "         0.0084, 0.0039, 0.1443, 0.7109, 0.4794, 0.4594, 0.6280, 0.5412, 0.8272],\n",
       "        [0.4077, 0.1372, 0.5140, 0.0969, 0.1731, 0.7660, 0.2043, 0.8372, 0.4805,\n",
       "         0.5406, 0.1567, 0.4494, 0.3641, 0.3795, 0.4450, 0.4580, 0.8718, 0.3630,\n",
       "         0.4909, 0.6181, 0.3448, 0.0902, 0.2372, 0.7876, 0.4160, 0.1699, 0.2071],\n",
       "        [0.4077, 0.1372, 0.5140, 0.0969, 0.1731, 0.7660, 0.2043, 0.8372, 0.4805,\n",
       "         0.5406, 0.1567, 0.4494, 0.3641, 0.3795, 0.4450, 0.4580, 0.8718, 0.3630,\n",
       "         0.4909, 0.6181, 0.3448, 0.0902, 0.2372, 0.7876, 0.4160, 0.1699, 0.2071],\n",
       "        [0.9135, 0.8817, 0.5788, 0.2431, 0.3961, 0.7055, 0.1113, 0.5795, 0.7377,\n",
       "         0.3734, 0.4681, 0.4328, 0.2073, 0.2705, 0.8959, 0.4624, 0.9733, 0.1387,\n",
       "         0.2958, 0.1278, 0.9244, 0.7165, 0.0402, 0.8755, 0.4809, 0.6072, 0.4759]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log counts = logits\n",
    "logits = xenc @ W\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.5513, 1.6803, 2.6550, 1.5660, 1.7426, 2.4548, 1.5731, 1.0491, 1.9698,\n",
       "         2.1916, 1.8922, 1.1689, 1.3679, 1.0243, 1.8289, 1.0454, 2.2296, 2.0069,\n",
       "         1.0090, 1.8216, 1.1852, 2.4195, 2.2382, 1.4119, 2.0368, 2.3740, 1.0089],\n",
       "        [1.1340, 1.2773, 1.2113, 1.0809, 2.6709, 2.7077, 1.1340, 2.7080, 2.1132,\n",
       "         1.0719, 2.5214, 2.5036, 1.4196, 2.6752, 1.8668, 1.0180, 2.5044, 1.0226,\n",
       "         1.0085, 1.0039, 1.1552, 2.0358, 1.6151, 1.5831, 1.8738, 1.7180, 2.2868],\n",
       "        [1.5033, 1.1471, 1.6720, 1.1018, 1.1890, 2.1511, 1.2266, 2.3098, 1.6169,\n",
       "         1.7170, 1.1696, 1.5674, 1.4393, 1.4615, 1.5606, 1.5809, 2.3912, 1.4376,\n",
       "         1.6338, 1.8554, 1.4116, 1.0944, 1.2677, 2.1981, 1.5159, 1.1852, 1.2301],\n",
       "        [1.5033, 1.1471, 1.6720, 1.1018, 1.1890, 2.1511, 1.2266, 2.3098, 1.6169,\n",
       "         1.7170, 1.1696, 1.5674, 1.4393, 1.4615, 1.5606, 1.5809, 2.3912, 1.4376,\n",
       "         1.6338, 1.8554, 1.4116, 1.0944, 1.2677, 2.1981, 1.5159, 1.1852, 1.2301],\n",
       "        [2.4931, 2.4149, 1.7838, 1.2752, 1.4860, 2.0248, 1.1177, 1.7852, 2.0912,\n",
       "         1.4527, 1.5970, 1.5415, 1.2304, 1.3107, 2.4495, 1.5878, 2.6466, 1.1488,\n",
       "         1.3441, 1.1364, 2.5203, 2.0472, 1.0410, 2.4000, 1.6175, 1.8353, 1.6095]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = logits.exp()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the rows (sum over 1st dim)\n",
    "probs = counts / counts.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Every row sums to 1\n",
    "probs[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0537, 0.0354, 0.0559, 0.0330, 0.0367, 0.0517, 0.0331, 0.0221, 0.0415,\n",
       "        0.0461, 0.0398, 0.0246, 0.0288, 0.0216, 0.0385, 0.0220, 0.0469, 0.0422,\n",
       "        0.0212, 0.0383, 0.0250, 0.0509, 0.0471, 0.0297, 0.0429, 0.0500, 0.0212])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first vector represents the \n",
    "#. e\n",
    "#e m\n",
    "#m m\n",
    "#m a\n",
    "#a .\" \n",
    "probs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary by Andrej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.rand((27, 27))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "\n",
    "# Count and probs taken together <==> softmax\n",
    "counts = logits.exp() # counts, quivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to interpret?\n",
    "\n",
    "The 'output probabilities from the neural net' gives all the probabilities for the 27 elements\n",
    "\n",
    "The 'label' is what we try to predict.\n",
    "\n",
    "The 'probability assigned by the net to the the correct character' is what the network believe is the probability of the label being the actual correct output. If this is low, it is not good. In this case, this is likely to happen because we haven't really trained the network yet.\n",
    "\n",
    "A high negative log likelihood is a bad score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "bigram example 1: .e (indexes: (0, 5))\n",
      "input to the neural net: 0\n",
      "output probabilities from the neural net:\n",
      "---\n",
      " tensor([0.0339, 0.0333, 0.0225, 0.0234, 0.0522, 0.0248, 0.0416, 0.0559, 0.0306,\n",
      "        0.0347, 0.0410, 0.0530, 0.0288, 0.0371, 0.0284, 0.0341, 0.0217, 0.0431,\n",
      "        0.0276, 0.0424, 0.0473, 0.0525, 0.0434, 0.0452, 0.0341, 0.0258, 0.0418])\n",
      "---\n",
      "label (actual next character): 5\n",
      "probability assigned by the net to the the correct character: 0.02480095811188221\n",
      "log likelihood: -3.6968729496002197\n",
      "negative log likelihood: 3.6968729496002197\n",
      "\n",
      "--------\n",
      "bigram example 2: em (indexes: (5, 13))\n",
      "input to the neural net: 5\n",
      "output probabilities from the neural net:\n",
      "---\n",
      " tensor([0.0324, 0.0341, 0.0450, 0.0525, 0.0269, 0.0378, 0.0558, 0.0277, 0.0554,\n",
      "        0.0283, 0.0304, 0.0297, 0.0222, 0.0500, 0.0303, 0.0232, 0.0260, 0.0324,\n",
      "        0.0500, 0.0441, 0.0441, 0.0424, 0.0436, 0.0234, 0.0476, 0.0309, 0.0339])\n",
      "---\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.05002150684595108\n",
      "log likelihood: -2.995302200317383\n",
      "negative log likelihood: 2.995302200317383\n",
      "\n",
      "--------\n",
      "bigram example 3: mm (indexes: (13, 13))\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net:\n",
      "---\n",
      " tensor([0.0366, 0.0380, 0.0257, 0.0425, 0.0376, 0.0479, 0.0247, 0.0511, 0.0240,\n",
      "        0.0223, 0.0442, 0.0315, 0.0306, 0.0318, 0.0237, 0.0349, 0.0487, 0.0256,\n",
      "        0.0350, 0.0360, 0.0457, 0.0295, 0.0546, 0.0525, 0.0546, 0.0238, 0.0471])\n",
      "---\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.03175963833928108\n",
      "log likelihood: -3.449558973312378\n",
      "negative log likelihood: 3.449558973312378\n",
      "\n",
      "--------\n",
      "bigram example 4: ma (indexes: (13, 1))\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net:\n",
      "---\n",
      " tensor([0.0366, 0.0380, 0.0257, 0.0425, 0.0376, 0.0479, 0.0247, 0.0511, 0.0240,\n",
      "        0.0223, 0.0442, 0.0315, 0.0306, 0.0318, 0.0237, 0.0349, 0.0487, 0.0256,\n",
      "        0.0350, 0.0360, 0.0457, 0.0295, 0.0546, 0.0525, 0.0546, 0.0238, 0.0471])\n",
      "---\n",
      "label (actual next character): 1\n",
      "probability assigned by the net to the the correct character: 0.03795874863862991\n",
      "log likelihood: -3.2712552547454834\n",
      "negative log likelihood: 3.2712552547454834\n",
      "\n",
      "--------\n",
      "bigram example 5: a. (indexes: (1, 0))\n",
      "input to the neural net: 1\n",
      "output probabilities from the neural net:\n",
      "---\n",
      " tensor([0.0513, 0.0455, 0.0469, 0.0237, 0.0277, 0.0347, 0.0370, 0.0325, 0.0266,\n",
      "        0.0447, 0.0482, 0.0475, 0.0329, 0.0530, 0.0292, 0.0507, 0.0225, 0.0246,\n",
      "        0.0246, 0.0492, 0.0324, 0.0266, 0.0455, 0.0419, 0.0256, 0.0439, 0.0311])\n",
      "---\n",
      "label (actual next character): 0\n",
      "probability assigned by the net to the the correct character: 0.051290806382894516\n",
      "log likelihood: -2.9702436923980713\n",
      "negative log likelihood: 2.9702436923980713\n",
      "\n",
      "=========\n",
      "average negative log likelihood, i.e. loss = 3.276646375656128\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "  # i-th bigram:\n",
    "  x = xs[i].item() # input character index\n",
    "  y = ys[i].item() # label character index\n",
    "  print('--------')\n",
    "  print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes: ({x}, {y}))')\n",
    "  print('input to the neural net:', x)\n",
    "  print(f'output probabilities from the neural net:\\n---\\n {probs[i]}\\n---')\n",
    "  print('label (actual next character):', y)\n",
    "  p = probs[i, y]\n",
    "  print('probability assigned by the net to the the correct character:', p.item())\n",
    "  logp = torch.log(p)\n",
    "  print('log likelihood:', logp.item())\n",
    "  nll = -logp\n",
    "  print(f'negative log likelihood: {nll.item()}\\n')\n",
    "  nlls[i] = nll\n",
    "\n",
    "print('=========')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to change W, the scores (negative log likelihood) would change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.rand((27, 27), generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "\n",
    "# Count and probs taken together <==> softmax\n",
    "counts = logits.exp() # counts, quivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0229),\n",
       " tensor(0.0258),\n",
       " tensor(0.0339),\n",
       " tensor(0.0341),\n",
       " tensor(0.0402))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give [input, output] and see what the probability is for each\n",
    "probs[0, 5], probs[1, 13], probs[2, 13], probs[3, 1], probs[4, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same in a less verbose way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0229, 0.0258, 0.0339, 0.0341, 0.0402])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[torch.arange(5), ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4815)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -probs[torch.arange(5), ys].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean version #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize 27 neurons' weights. Each of these 27 neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.rand((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch builds a full computational graph, i.e. it keeps track of all the operations and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "\n",
    "# Count and probs taken together <==> softmax\n",
    "counts = logits.exp() # counts, quivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "loss = -probs[torch.arange(5), ys].log().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'.backward()' will fill-in the gradients, all the way back to W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.grad = None # Set to zero the gradient\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4815192222595215\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0085,  0.0060,  0.0047,  0.0076,  0.0046, -0.1954,  0.0101,  0.0113,\n",
       "          0.0042,  0.0056,  0.0088,  0.0063,  0.0099,  0.0088,  0.0062,  0.0052,\n",
       "          0.0085,  0.0113,  0.0109,  0.0065,  0.0085,  0.0044,  0.0094,  0.0057,\n",
       "          0.0113,  0.0059,  0.0052],\n",
       "        [-0.1920,  0.0103,  0.0061,  0.0080,  0.0052,  0.0064,  0.0079,  0.0060,\n",
       "          0.0062,  0.0110,  0.0094,  0.0080,  0.0078,  0.0094,  0.0116,  0.0077,\n",
       "          0.0068,  0.0080,  0.0049,  0.0067,  0.0046,  0.0047,  0.0098,  0.0062,\n",
       "          0.0070,  0.0051,  0.0073],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0060,  0.0056,  0.0048,  0.0062,  0.0060,  0.0072,  0.0080,  0.0054,\n",
       "          0.0108,  0.0089,  0.0112,  0.0048,  0.0072, -0.1948,  0.0074,  0.0108,\n",
       "          0.0055,  0.0103,  0.0053,  0.0051,  0.0063,  0.0059,  0.0063,  0.0101,\n",
       "          0.0111,  0.0112,  0.0076],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0127, -0.1864,  0.0156,  0.0137,  0.0139,  0.0087,  0.0097,  0.0113,\n",
       "          0.0186,  0.0215,  0.0192,  0.0104,  0.0208, -0.1864,  0.0180,  0.0177,\n",
       "          0.0091,  0.0122,  0.0138,  0.0217,  0.0088,  0.0196,  0.0115,  0.0094,\n",
       "          0.0219,  0.0125,  0.0206],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Version #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "    \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=0 : loss=2.4873\n",
      "iteration=1 : loss=2.4872\n",
      "iteration=2 : loss=2.4871\n",
      "iteration=3 : loss=2.4870\n",
      "iteration=4 : loss=2.4869\n",
      "iteration=5 : loss=2.4868\n",
      "iteration=6 : loss=2.4867\n",
      "iteration=7 : loss=2.4866\n",
      "iteration=8 : loss=2.4865\n",
      "iteration=9 : loss=2.4864\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(10):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
    "  print(f'iteration={k} : loss={loss.item():.4f}')\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juwjdjdjancqydjufhqyywecnw.\n",
      ".\n",
      "oiin.\n",
      "toziasz.\n",
      "twt.\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    \n",
    "    # ----------\n",
    "    # BEFORE:\n",
    "    #p = P[ix]\n",
    "    # ----------\n",
    "    # NOW:\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "    \n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
